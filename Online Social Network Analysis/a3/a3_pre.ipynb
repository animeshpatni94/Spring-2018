{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # Assignment 3:  Recommendation systems\n",
    "#\n",
    "# Here we'll implement a content-based recommendation algorithm.\n",
    "# It will use the list of genres for a movie as the content.\n",
    "# The data come from the MovieLens project: http://grouplens.org/datasets/movielens/\n",
    "# Note that I have not provided many doctests for this one. I strongly\n",
    "# recommend that you write your own for each function to ensure your\n",
    "# implementation is correct.\n",
    "\n",
    "# Please only use these imports.\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "def download_data():\n",
    "    \"\"\" DONE. Download and unzip data.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/h9ubx22ftdkyvd5/ml-latest-small.zip?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'ml-latest-small.zip')\n",
    "    zfile = zipfile.ZipFile('ml-latest-small.zip')\n",
    "    zfile.extractall()\n",
    "    zfile.close()\n",
    "\n",
    "\n",
    "def tokenize_string(my_string):\n",
    "    \"\"\" DONE. You should use this in your tokenize function.\n",
    "    \"\"\"\n",
    "    return re.findall('[\\w\\-]+', my_string.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. Use the tokenize_string method above.\n",
    "    Note: you may modify the movies parameter directly; no need to make\n",
    "    a new copy.\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "    >>> movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "    >>> movies = tokenize(movies)\n",
    "    >>> movies['tokens'].tolist()\n",
    "    [['horror', 'romance'], ['sci-fi']]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    t=[]\n",
    "    for i in movies[\"genres\"]:\n",
    "        z = tokenize_string(i)\n",
    "        t.append(z)\n",
    "    #movies[\"tokens\"] = t\n",
    "    s = np.array(t)\n",
    "    #print(s)\n",
    "    s1 = pd.Series(s,movies.index)\n",
    "    #print(s1)\n",
    "    movies[\"tokens\"] = s1\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['horror', 'romance'], ['sci-fi']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "movies = tokenize(movies)\n",
    "movies['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term, as\n",
    "    defined in class:\n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "    \"\"\"\n",
    "    feats = defaultdict(lambda: 0)\n",
    "    f_list = list(defaultdict(lambda: 0))\n",
    "    ind = movies.index\n",
    "    shape = movies.shape[0]\n",
    "    mat=[]\n",
    "    mat_list = []\n",
    "    doc = []\n",
    "    r = []\n",
    "    c = []\n",
    "    d = []\n",
    "    mo = movies['tokens']\n",
    "    for i in mo:\n",
    "        w = 0\n",
    "        for j in set(i):\n",
    "            feats[j]=feats[j]+1\n",
    "            w=w+1\n",
    "    vocab = dict((i, sorted(feats).index(i)) for i in sorted(feats))\n",
    "    \n",
    "    new_dict = {}\n",
    "    for i in mo:\n",
    "        new_dict = defaultdict(lambda: 0)\n",
    "        for k in i:\n",
    "            new_dict[k] = new_dict[k]+1\n",
    "        f_list.append(new_dict)\n",
    "    \n",
    "    i=0\n",
    "    while i<shape:\n",
    "        r=[]\n",
    "        d=[]\n",
    "        c=[]\n",
    "        t=True\n",
    "        doc = f_list[i]\n",
    "        max_k = doc[max(doc,key=doc.get)]\n",
    "        for j in doc:\n",
    "            if j in vocab:\n",
    "                c.append(vocab[j])\n",
    "                w1 = doc[j]/max_k\n",
    "                w = math.log((shape / feats[j]),10)\n",
    "                tf_idf = w1 * w\n",
    "                d.append(tf_idf)\n",
    "        r = [0]*len(c)\n",
    "        l1 = len(vocab)\n",
    "        mat = csr_matrix((d,(r,c)),shape = (1, l1))\n",
    "        mat_list.append(mat)\n",
    "        i=i+1\n",
    "    f =pd.DataFrame(mat_list,ind)\n",
    "    movies['features'] = f\n",
    "    return movies,vocab\n",
    "\n",
    "            \n",
    "        \n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys -  ['action', 'comedy', 'horror', 'romance', 'sci-fi']\n",
      "\n",
      "vocab:\n",
      "[('action', 0), ('comedy', 1), ('horror', 2), ('romance', 3), ('sci-fi', 4)]\n"
     ]
    }
   ],
   "source": [
    "movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi|Action'],[789,'Comedy']],columns=['movieId', 'genres'])\n",
    "movies = tokenize(movies)\n",
    "vocab_keys = sorted(set(sum(movies['tokens'].tolist(), [])))\n",
    "print ('keys - ',vocab_keys)\n",
    "movies, vocab = featurize(movies)\n",
    "print('\\nvocab:')\n",
    "print(sorted(vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    \"\"\"DONE.\n",
    "    Returns a random split of the ratings matrix into a training and testing set.\n",
    "    \"\"\"\n",
    "    test = set(range(len(ratings))[::1000])\n",
    "    train = sorted(set(range(len(ratings))) - test)\n",
    "    test = sorted(test)\n",
    "    return ratings.iloc[train], ratings.iloc[test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two 1-d csr_matrices.\n",
    "    Each matrix represents the tf-idf feature vector of a movie.\n",
    "    Params:\n",
    "      a...A csr_matrix with shape (1, number_features)\n",
    "      b...A csr_matrix with shape (1, number_features)\n",
    "    Returns:\n",
    "      A float. The cosine similarity, defined as: dot(a, b) / ||a|| * ||b||\n",
    "      where ||a|| indicates the Euclidean norm (aka L2 norm) of vector a.\n",
    "    \"\"\"\n",
    "    A = a.dot(b.T).toarray()[0][0]\n",
    "    B = (np.linalg.norm(a.toarray())*np.linalg.norm(b.toarray()))\n",
    "    return A/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(movies, ratings_train, ratings_test):\n",
    "    \"\"\"\n",
    "    movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi|Action'],[789,'Comedy']], columns=['movieId', 'genres'])\n",
    "    ratings = pd.DataFrame([[1,123,2],[1,456,4],[1,789,1.5],[2,123,3],[2,456,1],[2,789,3],[3,123,3],[3,456,4],[3,789,2],[4,123,2],[4,456,3],[4,789,2.5],\n",
    "                            [5,123,2.5],[5,456,4.5],[5,789,5],[6,123,3.5],[6,456,1.5],[6,789,5],[7,123,3.5],[7,456,4.5],[7,789,4],[8,123,2.5],[8,456,3.5],[8,789,4.5],\n",
    "                            [9,123,4],[9,456,4],[9,789,2.5],[10,123,3],[10,456,2],[10,789,4],[11,123,3],[11,456,4],[11,789,3.5],[12,123,2],[12,456,1],[12,789,3]],\n",
    "                           columns=['userId','movieId','rating'])\n",
    "    movies = tokenize(movies)\n",
    "    vocab_keys = sorted(set(sum(movies['tokens'].tolist(), [])))\n",
    "    print ('keys - ',vocab_keys)\n",
    "    movies, vocab = featurize(movies)\n",
    "    print('vocab:')\n",
    "    print(sorted(vocab.items())[:10])\n",
    "    ratings_train, ratings_test = train_test_split(ratings)\n",
    "\n",
    "    print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "    predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "\n",
    "    #Out\n",
    "    keys -  ['action', 'comedy', 'horror', 'romance', 'sci-fi']\n",
    "\n",
    "    vocab:\n",
    "    [('action', 0), ('comedy', 1), ('horror', 2), ('romance', 3), ('sci-fi', 4)]\n",
    "    35 training ratings; 1 testing ratings\n",
    "    [ 2.75]\n",
    "    \"\"\"\n",
    "    r=[]\n",
    "    iterr = ratings_test.iterrows()\n",
    "    for i,j in iterr:\n",
    "        mid = j['movieId']\n",
    "        feat = movies.loc[movies.movieId == mid].squeeze()['features']\n",
    "        wr = 0.\n",
    "        ss = 0.\n",
    "        a = True\n",
    "        uid = j['userId']\n",
    "        user = ratings_train.loc[ratings_train.userId == uid]\n",
    "        u_iterr = user.iterrows()\n",
    "        for k,l in u_iterr:\n",
    "            co_sine = cosine_sim(feat,movies.loc[movies.movieId==l['movieId']].squeeze()['features'])\n",
    "            #if co_sine < 0:\n",
    "            if co_sine > 0:\n",
    "                ss = ss + co_sine\n",
    "                wr = wr + co_sine * l['rating']\n",
    "                a = False\n",
    "        if a != True:\n",
    "            r.append(wr/ss)\n",
    "        elif a == True:\n",
    "            r.append(user['rating'].mean())\n",
    "    \n",
    "    z = np.array(r)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys -  ['action', 'comedy', 'horror', 'romance', 'sci-fi']\n",
      "vocab:\n",
      "[('action', 0), ('comedy', 1), ('horror', 2), ('romance', 3), ('sci-fi', 4)]\n",
      "35 training ratings; 1 testing ratings\n"
     ]
    }
   ],
   "source": [
    "movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi|Action'],[789,'Comedy']], columns=['movieId', 'genres'])\n",
    "ratings = pd.DataFrame([[1,123,2],[1,456,4],[1,789,1.5],[2,123,3],[2,456,1],[2,789,3],[3,123,3],[3,456,4],[3,789,2],[4,123,2],[4,456,3],[4,789,2.5],\n",
    "                            [5,123,2.5],[5,456,4.5],[5,789,5],[6,123,3.5],[6,456,1.5],[6,789,5],[7,123,3.5],[7,456,4.5],[7,789,4],[8,123,2.5],[8,456,3.5],[8,789,4.5],\n",
    "                            [9,123,4],[9,456,4],[9,789,2.5],[10,123,3],[10,456,2],[10,789,4],[11,123,3],[11,456,4],[11,789,3.5],[12,123,2],[12,456,1],[12,789,3]],\n",
    "                           columns=['userId','movieId','rating'])\n",
    "movies = tokenize(movies)\n",
    "vocab_keys = sorted(set(sum(movies['tokens'].tolist(), [])))\n",
    "print ('keys - ',vocab_keys)\n",
    "movies, vocab = featurize(movies)\n",
    "print('vocab:')\n",
    "print(sorted(vocab.items())[:10])\n",
    "ratings_train, ratings_test = train_test_split(ratings)\n",
    "\n",
    "print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "predictions = make_predictions(movies, ratings_train, ratings_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(predictions, ratings_test):\n",
    "    \"\"\"DONE.\n",
    "    Return the mean absolute error of the predictions.\n",
    "    \"\"\"\n",
    "    return np.abs(predictions - np.array(ratings_test.rating)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab:\n",
      "[('action', 0), ('adventure', 1), ('animation', 2), ('children', 3), ('comedy', 4), ('crime', 5), ('documentary', 6), ('drama', 7), ('fantasy', 8), ('film-noir', 9)]\n",
      "99903 training ratings; 101 testing ratings\n",
      "error=0.787455\n",
      "[2.7945653  2.62385286 2.76558239 4.24064843 3.22115155 4.11423684\n",
      " 3.92040563 3.98286924 3.21565818 3.32692774]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    download_data()\n",
    "    path = 'ml-latest-small'\n",
    "    ratings = pd.read_csv(path + os.path.sep + 'ratings.csv')\n",
    "    movies = pd.read_csv(path + os.path.sep + 'movies.csv')\n",
    "    movies = tokenize(movies)\n",
    "    movies, vocab = featurize(movies)\n",
    "    print('vocab:')\n",
    "    print(sorted(vocab.items())[:10])\n",
    "    ratings_train, ratings_test = train_test_split(ratings)\n",
    "    print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "    predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "    print('error=%f' % mean_absolute_error(predictions, ratings_test))\n",
    "    print(predictions[:10])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
