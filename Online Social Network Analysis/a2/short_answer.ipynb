{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anime\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best cross-validation result:\n",
      "{'punct': True, 'features': (<function token_features at 0x000001F9E693FBF8>, <function token_pair_features at 0x000001F9E871E730>), 'min_freq': 5, 'accuracy': 0.8}\n",
      "worst cross-validation result:\n",
      "{'punct': True, 'features': (<function lexicon_features at 0x000001F9E87DFAE8>,), 'min_freq': 2, 'accuracy': 0.6475}\n",
      "\n",
      "Mean Accuracies per Setting:\n",
      "features=token_features token_pair_features: 0.79000\n",
      "features=token_features token_pair_features lexicon_features: 0.78875\n",
      "features=token_features lexicon_features: 0.75208\n",
      "features=token_features: 0.75042\n",
      "punct=False: 0.74643\n",
      "min_freq=5: 0.74446\n",
      "min_freq=2: 0.74321\n",
      "punct=True: 0.73476\n",
      "min_freq=10: 0.73411\n",
      "features=token_pair_features lexicon_features: 0.72625\n",
      "features=token_pair_features: 0.72542\n",
      "features=lexicon_features: 0.65125\n",
      "\n",
      "TOP COEFFICIENTS PER CLASS:\n",
      "negative words:\n",
      "token=waste: 0.13692\n",
      "token=worst: 0.11197\n",
      "token_pair=far__too: 0.10840\n",
      "token=boring: 0.10662\n",
      "token_pair=the__worst: 0.10488\n",
      "\n",
      "positive words:\n",
      "token=excellent: 0.12287\n",
      "token_pair=a__beautiful: 0.11044\n",
      "token_pair=is__great: 0.09901\n",
      "token=great: 0.09603\n",
      "token=worth: 0.09518\n",
      "testing accuracy=0.730000\n",
      "\n",
      "TOP MISCLASSIFIED TEST DOCUMENTS:\n",
      "truth=0 predicted=1 proba=0.943479\n",
      "Steven Seagal is back! Here with his third film released this year. Of course as a one time fan who has become increasingly disgruntled I can say it comes as no surprise that this is pretty lame. Firstly the film made headlines because of apparent problems in production due to Seagal. He would turn up late on set, change the script, crew etc and generally cause problems for the director, Don E Faun Leroy (his lack of talent his trouble enough!). This also happens in their second collaboration the upcoming Mercenary which promises to be just as bad as this garbage. This also marks a big turning point in Seagal's career because this film is the first of his to really dig out the stock footage. There was a little in Ticker but this film takes the biscuit. They borrow bits from, The Order (A Van Damme movie, Seagals biggest rival in DTV movies!) No Code Of Conduct, Undisputed, and also an entire action sequence from the little known Peter Weller starring vehicle Top Of The World. Interestingly the car chase stolen from Weller's epic, made almost 10 years ago and ironically probably cheaper than this garbage, is actually by far the best action scene of the film. I was shocked enough when Dolph Lundgren had a brief stint in the stock action video world, which thankfully he has escaped from. Seagal though is the leader of the DTV action market currently, with Van Damme and Snipes his main rivals. Seagal still manages to sell movies and for the life of me I don't know how. Surely the fans must be getting bored of this awfulness, longing for a return to the likes of Above The Law. The story here is totally lame. In fact the film has so many plot holes it doesn't bare thinking about. For example at the end of a film there's a little girl that Seagal apparently knows at an orphanage who he gives a necklace to. Why I don't know but we never see her at all in the rest of the movie, or hear her mentioned. Seagal has a girlfriend in this movie who at the beginning of the film is with a psychic and she becomes haunted by visions, which by the end of the film are never explained and mean nothing. The film is so ridiculously glued together by a series of meaningless pap that it becomes headache inducing.. This is by far Seagal's dumbest movie! Seagal himself is as wooden as ever, however to his credit he doesn't get dubbed in this one as far as I could tell. Seagal does however feel the need to talk like he is a gangsta rapper, making me long for the days he would don his Brooklyn/Italian-American accent, in his classic early films. He also has a painfully unfunny double act with Treach, who I assume is a rapper. It is funny how producers seem to think that the combo of Seagal, plus hip-hop star seems to work, because his team up with DMX in Exit Wounds was his most successful film since Under Siege. Clearly though if no one has heard of the rapper, it won't work. This is an action film though and so the action itself must be judged. Unfortunately the action that didn't come form the NU Image back catalogue is strictly routine. There are a few small fight scenes with some classic Seagal aikido but when 90% is performed by his stunt double, who really does have a rigorous work out in this film, it really doesn't impress much. There are also some standard gun fights which really only have some nice violent and bloody squibbage going for them. All in all this is a painfully boring experience and once again I'm left giving the same verdict: Seagal has lost it! I keep asking the years old question now, \"why do people still watch his movies?\" That is all very well and good as a question but the sad bastard that I am continues to watch his films in the deluded hope he may do something good once again. Chances are slim, unlike Seagal's ever expanding waistline. *1/2\n",
      "\n",
      "truth=1 predicted=0 proba=0.907078\n",
      "In defense of this movie I must repeat what I had stated previously. The movie is called Arachina, it has a no name cast and I do not mean no name as in actors who play in little seen art house films. I mean no name as in your local high school decided to make a film no name and it might have a 2 dollar budget. So what does one expect? Hitchcock?<br /><br />I felt the movie never took itself seriously which automatically takes it out of the worst movie list. That list is only for big budget all star cast movies that takes itself way too seriously. THe movie The Oscar comes to mind, most of Sylvester Stallone's movies. THe two leads were not Hepburn and Tracy but they did their jobs well enough for this movie. The woman kicked butt and the guy was not a blithering idiot. The actor who played the old man was actually very good. The man who played anal retentive professor was no Clifton Webb but he did a god job. And the Bimbo's for lack of a better were played by two competent actors. I laughed at the 50 cent special effects. But that was part of the charm of the movie. It played like a hybrid Tremors meets Night of the Living Dead. The premise of the movie is just like all Giant Bug movies of the 50's. A Meteor or radiation stir up the ecosystem and before you know it we have Giant Ants, Lobsters, rocks or Lizards terrorizing the locals. A meteor was the cause of the problems this time. I was was very entertained. I didn't expect much and I go a lot more then I bargained for.\n",
      "\n",
      "truth=0 predicted=1 proba=0.906752\n",
      "I absolutely despise this film. I wanted to love it - I really wanted to. But man, oh man - they were SO off with Sara. And the father living was pretty cheesy. That's straight out of the Shirley Temple film.<br /><br />I highly recommend THE BOOK. It is amazing. In the book, Sara is honorable and decent and she does the right thing... BECAUSE IT IS RIGHT. She doesn't have a spiteful bone in her body.<br /><br />In the film, she is mean-spirited and spiteful. She does little things to get back at Miss Minchin. In the book, Sara is above such things. She DOES stand up to Miss Minchin. She tells the truth and is not cowed by her. But she does not do the stupid, spiteful things that the Sara in the film does.<br /><br />It's really rather unsettling to me that so many here say they loved the book and they love the movie. I can't help but wonder... did we read the same book? The whole point of the book was personal responsibility, behaving with honor and integrity, ALWAYS telling the truth and facing adversity with calm and integrity.<br /><br />Sara has a happy ending in the book - not the ridiculous survival of her father, but the joining with his partner who has been searching for her. In the book, she is taken in by this new father figure who loves and cares for her and Becky. And Miss Minchin is NOT a chimney sweep - that part of the film really was stupid.<br /><br />To see all this praise for this wretched film is disturbing to me. We are praising a film that glorifies petty, spiteful behavior with a few tips of the hat to kindness? Sara in the book was kind to the bone and full of integrity. I don't even recognize her in the film... she's not in it.<br /><br />Good thing Mrs. Burnett isn't alive to see this horrid thing. It's ghastly and undeserving to bear the title of her book.\n",
      "\n",
      "truth=0 predicted=1 proba=0.879842\n",
      "The plot for Black Mama White Mama, revolves around two female inmates, at a women's prison in the Phillipines. One Black, and one White. These two women, are thrown together in the prison. Pam Grier is Lee Daniels Lee is incarcerated in the hellish women's prison, for dancing as a harem girl. <br /><br />Lee's boyfriend owes her part of his profits, from his drug-dealing activities. Lee is mainly interested in breaking out of the prison to get hold of her beau's drug money, so that she can leave the Phillipines and assume a better life. Margaret Markov plays Karen Brent, a white women from a privileged background, who is also a revolutionary. Karen has joined a group of revolutionaries, determined to change the corrupt Phillipino political system. She's captured by Phillipino authorities, and held as a political prisoner.<br /><br />The story-line takes-off, when Karen and Lee break out of the prison they were in together. The two of them also happened to be chained together at the wrist. As they flee, they also fight with each other, because they have different goals to pursue. Naturally, they hate being chained together. But they also realize that they must put aside their differences, to help each other survive while they evade capture.<br /><br />If this film seems very similar to The Big Bird Cage, it's because much of the cast in the two films is the same, as well as their location in the Phillipines. Roger Corman, has always had a consistent stable of actors, that he used in all of his 70s B movies. Besides Pam Grier, Sid Haig, Roberta Collins, Claudia Jennings, Betty Anne Rees, and William Smith, were also among the many actors that were frequently cast, in Corman's AIP films.<br /><br />Like The Big Bird Cage, Black Mama White Mama, relies on too much gory violence to be palatable. Pam Grier conveys her usual tough chick persona in this film, and shows her competence as a female action heroine. Margaret Markov is less effect, in her portrayal of the revolutionary Karen. She just seems to fragile and well-coiffed, to be a dedicated political guerrilla. Except for Sid Haig, as the colorful Ruben, the rest of the cast is forgettable.<br /><br />This film has little entertainment value, unless excessive, heinous acts of violence are your thing. Only the performances by Pam Grier and Sig Haig, make this film worth watching.\n",
      "\n",
      "truth=1 predicted=0 proba=0.875185\n",
      "This movie blew me away - I have only seen two episodes of the show, never saw the first movie, but went to a pre-screening where Johnny Knoxville himself introduced the movie, telling us to 'turn off our sense of moral judgment for an hour and a half.' He was right. As a movie, this would probably rate a 2, given it has zero plot, no structure besides randomness, and very little production value. However, that isn't the point. Everyone in our theatre was laughing and gasping the whole way through - not only were some of the stunts creative (see trailer if you need to know but they hid some of the best (or worst depending on how you want to look at it)), but some of the stuff they did took us completely by surprise. These guys do some stuff that won't make it into your newspaper reviews (and probably can't even be published here), involving lots of things below the belt. However, almost 3/4 of the stunts are fantastically hysterical (even if morally condemnable, but remember Knoxville's statement), and if you are in the right mindset this movie is hysterical to watch. Only about 20 minutes of this movie could have actually been shown on TV, so consider yourself warned of what you're getting into - some stuff is disgusting, but instead of being repulsed by it you end up laughing at the sheer stupidity of it all. As a person who thought Jackass the TV show was an over-hyped fad with only a few funny sketches and lots of unnecessary pain, the amount of fun I had at this movie has made me realize that having no boundaries is the best environment for these guys to work in. It's a lot of fun and should be a great comedic fix until the Borat movie comes out. With this movie, you may think you know what you're getting, but these guys are a few steps ahead of you - I guarantee you'll be surprised by the 3rd sketch. So enjoy, and don't worry: you won't want to perform almost any of their stuff at home.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f9e0d7b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# No imports allowed besides these.\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def download_data():\n",
    "    \"\"\" Download and unzip data.\n",
    "    DONE ALREADY.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/xk4glpk61q3qrg2/imdb.tgz?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'imdb.tgz')\n",
    "    tar = tarfile.open(\"imdb.tgz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    \n",
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Walks all subdirectories of this path and reads all\n",
    "    the text files and labels.\n",
    "    DONE ALREADY.\n",
    "    Params:\n",
    "      path....path to files\n",
    "    Returns:\n",
    "      docs.....list of strings, one per document\n",
    "      labels...list of ints, 1=positive, 0=negative label.\n",
    "               Inferred from file path (i.e., if it contains\n",
    "               'pos', it is 1, else 0)\n",
    "    \"\"\"\n",
    "    fnames = sorted([f for f in glob.glob(os.path.join(path, 'pos', '*.txt'))])\n",
    "    data = [(1, open(f).readlines()[0]) for f in sorted(fnames)]\n",
    "    fnames = sorted([f for f in glob.glob(os.path.join(path, 'neg', '*.txt'))])\n",
    "    data += [(0, open(f).readlines()[0]) for f in sorted(fnames)]\n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "    return np.array([d[1] for d in data]), np.array([d[0] for d in data])\n",
    "\n",
    "def tokenize(doc, keep_internal_punct=False):\n",
    "    \"\"\"\n",
    "    Tokenize a string.\n",
    "    The string should be converted to lowercase.\n",
    "    If keep_internal_punct is False, then return only the alphanumerics (letters, numbers and underscore).\n",
    "    If keep_internal_punct is True, then also retain punctuation that\n",
    "    is inside of a word. E.g., in the example below, the token \"isn't\"\n",
    "    is maintained when keep_internal_punct=True; otherwise, it is\n",
    "    split into \"isn\" and \"t\" tokens.\n",
    "    Params:\n",
    "      doc....a string.\n",
    "      keep_internal_punct...see above\n",
    "    Returns:\n",
    "      a numpy array containing the resulting tokens.\n",
    "    >>> tokenize(\" Hi there! Isn't this fun?\", keep_internal_punct=False)\n",
    "    array(['hi', 'there', 'isn', 't', 'this', 'fun'], \n",
    "          dtype='<U5')\n",
    "    >>> tokenize(\"Hi there! Isn't this fun? \", keep_internal_punct=True)\n",
    "    array(['hi', 'there', \"isn't\", 'this', 'fun'], \n",
    "          dtype='<U5')\n",
    "    \"\"\"\n",
    "    w=\"\"\n",
    "    punch = string.punctuation.replace(\"_\",\"\")\n",
    "    if(keep_internal_punct==False):\n",
    "        result = re.sub(\"\\W+\",\" \",doc.lower()).split()\n",
    "        final = np.array(result, dtype=\"unicode\")\n",
    "    else:\n",
    "        for i in doc.split():\n",
    "            #punch = punctuation.replace(\"_\",\"\")\n",
    "            r = i.strip(punch)\n",
    "            w = w + r + \" \"\n",
    "        result = re.sub(\"^\\W+\",\" \",w.lower()).split()\n",
    "        final = np.array(result, dtype=\"unicode\")\n",
    "    return final\n",
    "\n",
    "def token_features(tokens, feats):\n",
    "    \"\"\"\n",
    "    Add features for each token. The feature name\n",
    "    is pre-pended with the string \"token=\".\n",
    "    Note that the feats dict is modified in place,\n",
    "    so there is no return value.\n",
    "    Params:\n",
    "      tokens...array of token strings from a document.\n",
    "      feats....dict from feature name to frequency\n",
    "    Returns:\n",
    "      nothing; feats is modified in place.\n",
    "    >>> feats = defaultdict(lambda: 0)\n",
    "    >>> token_features(['hi', 'there', 'hi'], feats)\n",
    "    >>> sorted(feats.items())\n",
    "    [('token=hi', 2), ('token=there', 1)]\n",
    "    \"\"\"\n",
    "    c=0\n",
    "    for i in tokens:\n",
    "        c=0\n",
    "        for j in tokens:\n",
    "            if i==j:\n",
    "                c=c+1\n",
    "        feats[\"token=\"+i] = c\n",
    "        \n",
    "def token_pair_features(tokens, feats, k=3):\n",
    "    \"\"\"\n",
    "    Compute features indicating that two words occur near\n",
    "    each other within a window of size k.\n",
    "    For example [a, b, c, d] with k=3 will consider the\n",
    "    windows: [a,b,c], [b,c,d]. In the first window,\n",
    "    a_b, a_c, and b_c appear; in the second window,\n",
    "    b_c, c_d, and b_d appear. This example is in the\n",
    "    doctest below.\n",
    "    Note that the order of the tokens in the feature name\n",
    "    matches the order in which they appear in the document.\n",
    "    (e.g., a__b, not b__a)\n",
    "    Params:\n",
    "      tokens....array of token strings from a document.\n",
    "      feats.....a dict from feature to value\n",
    "      k.........the window size (3 by default)\n",
    "    Returns:\n",
    "      nothing; feats is modified in place.\n",
    "    >>> feats = defaultdict(lambda: 0)\n",
    "    >>> token_pair_features(np.array(['a', 'b', 'c', 'd']), feats)\n",
    "    >>> sorted(feats.items())\n",
    "    [('token_pair=a__b', 1), ('token_pair=a__c', 1), ('token_pair=b__c', 2), ('token_pair=b__d', 1), ('token_pair=c__d', 1)]\n",
    "    \"\"\"\n",
    "    #tpf = []\n",
    "    #r = len(tokens)-k+1\n",
    "    #for x in range(r):\n",
    "    #    for y in combinations(tokens[x:x+k],2):\n",
    "    #   tpf.extend([\"=\".join([\"token_pair\",\"__\".join(y)])])\n",
    "    w=[]\n",
    "    #z={}\n",
    "    for i in range(len(tokens)):\n",
    "        c=0\n",
    "        j=i\n",
    "        if i+k-1<len(tokens):\n",
    "            while(c<k):\n",
    "                w.append(tokens[j])\n",
    "                c=c+1\n",
    "                j=j+1\n",
    "        else:\n",
    "            break\n",
    "        combination(w,feats)\n",
    "        #print(i)\n",
    "        #z[i]=w\n",
    "        #print(w)\n",
    "        w.clear()\n",
    "    #print(z)\"\"\"\n",
    "        \n",
    "def combination(w,feats): \n",
    "    #for i in combinations(w,2):\n",
    "    #        token = [i[0]+\"__\"+i[1]]\n",
    "    #        for j in token:\n",
    "    #            for k in token:\n",
    "    #                if j==k:\n",
    "    #                    c=c+1\n",
    "    #            feats[\"token=\"+j] = c\n",
    "            #feats[token] = feats[token]+1\n",
    "    #print(token)\n",
    "    #print(feats)\n",
    "    for i in combinations(w,2):\n",
    "        parts = [i[0]+\"__\"+i[1]]\n",
    "        #print(parts)\n",
    "        #feats[parts]=feats[parts]+1\n",
    "        for j in parts:\n",
    "            if \"token_pair=\"+j not in feats:\n",
    "                feats[\"token_pair=\"+j] = 1\n",
    "            elif \"token_pair=\"+j in feats:\n",
    "                feats[\"token_pair=\"+j] = feats[\"token_pair=\"+j] + 1\n",
    "    #print(feats)\n",
    "    #for i in parts:\n",
    "    #    c=0\n",
    "    #    for j in parts:\n",
    "    #        if i==j:\n",
    "    #            c=c+1\n",
    "    #    feats[\"token_pair=\"+i] = c\n",
    "    #print()\n",
    "    \n",
    "\n",
    "neg_words = set(['bad', 'hate', 'horrible', 'worst', 'boring'])\n",
    "pos_words = set(['awesome', 'amazing', 'best', 'good', 'great', 'love', 'wonderful'])\n",
    "\n",
    "\n",
    "def lexicon_features(tokens, feats):\n",
    "    \"\"\"\n",
    "    Add features indicating how many time a token appears that matches either\n",
    "    the neg_words or pos_words (defined above). The matching should ignore\n",
    "    case.\n",
    "    Params:\n",
    "      tokens...array of token strings from a document.\n",
    "      feats....dict from feature name to frequency\n",
    "    Returns:\n",
    "      nothing; feats is modified in place.\n",
    "    In this example, 'LOVE' and 'great' match the pos_words,\n",
    "    and 'boring' matches the neg_words list.\n",
    "    >>> feats = defaultdict(lambda: 0)\n",
    "    >>> lexicon_features(np.array(['i', 'LOVE', 'this', 'great', 'boring', 'movie']), feats)\n",
    "    >>> sorted(feats.items())\n",
    "    [('neg_words', 1), ('pos_words', 2)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    feats['pos_words'] = 0\n",
    "    feats['neg_words'] = 0\n",
    "    for i in tokens:\n",
    "        for j in pos_words:\n",
    "            if(i.lower()==j.lower()):\n",
    "                feats['pos_words'] = feats['pos_words']+1\n",
    "                \n",
    "    for i in tokens:\n",
    "        for j in neg_words:\n",
    "            if(i.lower()==j.lower()):\n",
    "                feats['neg_words'] = feats['neg_words']+1\n",
    "    pass\n",
    "\n",
    "\n",
    "def featurize(tokens, feature_fns):\n",
    "    \"\"\"\n",
    "    Compute all features for a list of tokens from\n",
    "    a single document.\n",
    "    Params:\n",
    "      tokens........array of token strings from a document.\n",
    "      feature_fns...a list of functions, one per feature\n",
    "    Returns:\n",
    "      list of (feature, value) tuples, SORTED alphabetically\n",
    "      by the feature name.\n",
    "    >>> feats = featurize(np.array(['i', 'LOVE', 'this', 'great', 'movie']), [token_features, lexicon_features])\n",
    "    >>> feats\n",
    "    [('neg_words', 0), ('pos_words', 2), ('token=LOVE', 1), ('token=great', 1), ('token=i', 1), ('token=movie', 1), ('token=this', 1)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    feats={}\n",
    "    w=[]\n",
    "    for i in feature_fns:\n",
    "        i(tokens,feats)\n",
    "        #print(feats)\n",
    "    for i,j in feats.items():\n",
    "        s=(i,j)\n",
    "        #print(s)\n",
    "        w.append(s)\n",
    "    return(sorted(w))\n",
    "\n",
    "\n",
    "def vectorize(tokens_list, feature_fns, min_freq, vocab=None):\n",
    "    \"\"\"\n",
    "    Given the tokens for a set of documents, create a sparse\n",
    "    feature matrix, where each row represents a document, and\n",
    "    each column represents a feature.\n",
    "    Params:\n",
    "      tokens_list...a list of lists; each sublist is an\n",
    "                    array of token strings from a document.\n",
    "      feature_fns...a list of functions, one per feature\n",
    "      min_freq......Remove features that do not appear in\n",
    "                    at least min_freq different documents.\n",
    "    Returns:\n",
    "      - a csr_matrix: See https://goo.gl/f5TiF1 for documentation.\n",
    "      This is a sparse matrix (zero values are not stored).\n",
    "      - vocab: a dict from feature name to column index. NOTE\n",
    "      that the columns are sorted alphabetically (so, the feature\n",
    "      \"token=great\" is column 0 and \"token=horrible\" is column 1\n",
    "      because \"great\" < \"horrible\" alphabetically),\n",
    "    >>> docs = [\"Isn't this movie great?\", \"Horrible, horrible movie\"]\n",
    "    >>> tokens_list = [tokenize(d) for d in docs]\n",
    "    >>> feature_fns = [token_features]\n",
    "    >>> X, vocab = vectorize(tokens_list, feature_fns, min_freq=1)\n",
    "    >>> type(X)\n",
    "    <class 'scipy.sparse.csr.csr_matrix'>\n",
    "    >>> X.toarray()\n",
    "    array([[1, 0, 1, 1, 1, 1],\n",
    "           [0, 2, 0, 1, 0, 0]], dtype=int64)\n",
    "    >>> sorted(vocab.items(), key=lambda x: x[1])\n",
    "    [('token=great', 0), ('token=horrible', 1), ('token=isn', 2), ('token=movie', 3), ('token=t', 4), ('token=this', 5)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    st = preprocessing.StandardScaler()\n",
    "    \n",
    "    if vocab != None:\n",
    "        d=[]\n",
    "        r=[]\n",
    "        c=[]\n",
    "        for i in range(len(tokens_list)):\n",
    "            r1 = dict(featurize((tokens_list[i]),feature_fns))\n",
    "            for j in r1:\n",
    "                for k in vocab:\n",
    "                    if j==k:\n",
    "                        r.append(i)\n",
    "                        c.append(vocab[j])\n",
    "                        d.append(r1[j])\n",
    "        x=len(tokens_list)\n",
    "        y=len(vocab)\n",
    "        result = csr_matrix((d,(r,c)), shape=(x,y))\n",
    "        X=result.toarray()\n",
    "        #print(X)\n",
    "        x_scalar = st.fit_transform(X)\n",
    "        return x_scalar,vocab\n",
    "    elif vocab == None:\n",
    "        new_vocab = defaultdict(list)\n",
    "        doc = {}\n",
    "        for i in range(len(tokens_list)):\n",
    "            r1 = dict(featurize((tokens_list[i]),feature_fns))\n",
    "            #r1 = dict(r)\n",
    "            doc[i]=r1\n",
    "            for j in r1:\n",
    "                #print(j)\n",
    "                new_vocab[j].append(i)\n",
    "        #print(new_vocab)\n",
    "        #vocab1 = {}\n",
    "        vocab1 = {}\n",
    "        index = 0\n",
    "        for i in sorted(new_vocab):\n",
    "            if(len(new_vocab[i])>=min_freq):\n",
    "                vocab1[i]=index\n",
    "                index = index + 1\n",
    "        #print(vocab1['token=great'])\n",
    "        #print(vocab1)\n",
    "        r=[]\n",
    "        c=[]\n",
    "        d=[]\n",
    "        for i in sorted(vocab1.keys()):\n",
    "            for j in sorted(new_vocab[i]):\n",
    "                if i in doc[j]:\n",
    "                    r.append(j)\n",
    "                    c.append(vocab1[i])\n",
    "                    d.append(doc[j][i])\n",
    "        x=len(tokens_list)\n",
    "        y=len(vocab1)\n",
    "        result = csr_matrix((d,(r,c)), shape=(x,y))\n",
    "        X=result.toarray()\n",
    "        #print(X)\n",
    "        x_scalar = st.fit_transform(X)\n",
    "        #print(result)\n",
    "        return x_scalar,vocab1\n",
    "    pass\n",
    "\n",
    "\n",
    "def accuracy_score(truth, predicted):\n",
    "    \"\"\" Compute accuracy of predictions.\n",
    "    DONE ALREADY\n",
    "    Params:\n",
    "      truth.......array of true labels (0 or 1)\n",
    "      predicted...array of predicted labels (0 or 1)\n",
    "    \"\"\"\n",
    "    return len(np.where(truth==predicted)[0]) / len(truth)\n",
    "\n",
    "def cross_validation_accuracy(clf, X, labels, k):\n",
    "    ###TODO\n",
    "    cv = KFold(n_splits = k,random_state=len(labels))\n",
    "    accuracies=[]\n",
    "    for train_ind, test_ind in cv.split(X):\n",
    "        #train, test = X[train_ind], X[test_ind]\n",
    "        clf.fit(X[train_ind],labels[train_ind])\n",
    "        predictions = clf.predict(X[test_ind])\n",
    "        accuracies.append(accuracy_score(labels[test_ind], predictions))\n",
    "    average = np.mean(np.array(accuracies))\n",
    "    #print(avergae)\n",
    "    return average\n",
    "    pass\n",
    "\n",
    "def eval_all_combinations(docs, labels, punct_vals,feature_fns, min_freqs):\n",
    "    feats = []\n",
    "    for x in range(1,len(feature_fns)+1):\n",
    "        for i in combinations(feature_fns,x):\n",
    "            feats.append(i)\n",
    "    final = []\n",
    "    for pv in punct_vals:\n",
    "        token = []\n",
    "        for d in docs:\n",
    "            token.append(tokenize(d,pv))\n",
    "        for m in min_freqs:\n",
    "            for f in feats:\n",
    "                X, vocab = vectorize(token, f, m)\n",
    "                #model = LogisticRegression()\n",
    "                zo=cross_validation_accuracy(LogisticRegression(), X, labels, 5)\n",
    "                #t={}\n",
    "                #t['punct'] = pv\n",
    "                #t['features'] = f\n",
    "                #t['min_freq'] = m\n",
    "                #t['accuracy'] = zo\n",
    "                #final.append(t)\n",
    "                final.append({'punct' : pv, 'features' : f, 'min_freq' : m, 'accuracy' : zo})\n",
    "    srt = sorted(final,key=lambda x:(x['accuracy'],x['min_freq']), reverse=True)\n",
    "    return srt\n",
    "\n",
    "\n",
    "def plot_sorted_accuracies(results):\n",
    "    ###TODO\n",
    "    a = []\n",
    "    for i in results:\n",
    "        a.append(i[\"accuracy\"])\n",
    "    srt = sorted(a)\n",
    "    plt.plot(srt)\n",
    "    plt.xlabel(\"setting\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.savefig(\"accuracies.png\")\n",
    "    pass\n",
    "\n",
    "\n",
    "def mean_accuracy_per_setting(results):\n",
    "    ###TODO\n",
    "    accuracy = []\n",
    "    f = []\n",
    "    for i in results:\n",
    "        accuracy.append((\"features\", i['features']))\n",
    "        accuracy.append((\"min_freq\", i['min_freq']))\n",
    "        accuracy.append((\"punct\", i['punct']))\n",
    "    for i in set(accuracy):\n",
    "        acc = []\n",
    "        for j in results:\n",
    "            if j[i[0]] == i[1]:\n",
    "                acc.append(j['accuracy'])\n",
    "                #print(acc)\n",
    "        if 'features' in i[0]:\n",
    "            fname = [k.__name__ for k in i[1]]\n",
    "            f.append((np.mean(acc), str((\"features=\" + \" \".join(fname)))))\n",
    "        else:\n",
    "            f.append((np.mean(acc), (i[0] + \"=\" + str(i[1]))))\n",
    "    srt = sorted(f, key=lambda x: -x[0])\n",
    "    return srt\n",
    "    #accuracy = []\n",
    "    #f = []\n",
    "    #for i in results:\n",
    "    #    f[\"min_freq=\"+str(result[\"min_freq\"])] = (temp[\"min_freq=\"+str(result)])\n",
    "    \n",
    "def fit_best_classifier(docs, labels, best_result):\n",
    "    ###TODO\n",
    "    t=[]\n",
    "    minfreq=best_result[\"min_freq\"]\n",
    "    features = best_result[\"features\"]\n",
    "    punct = best_result[\"punct\"]\n",
    "    for i in docs:\n",
    "        t.append(tokenize(i,punct))\n",
    "    X,v = vectorize(t,features,minfreq)\n",
    "    clf=LogisticRegression()\n",
    "    clf.fit(X,labels)\n",
    "    return clf,v\n",
    "    \n",
    "def top_coefs(clf, label, n, vocab):\n",
    "    ###TODO\n",
    "    r=[]\n",
    "    co = clf.coef_[0]\n",
    "    #going for neg\n",
    "    if label == 0:\n",
    "        for i in vocab:\n",
    "            r.append((i,co[vocab[i]]))\n",
    "        sort = sorted(r,key=lambda x: x[1])\n",
    "        r1 = []\n",
    "        for i in sort[:n]:\n",
    "            r1.append((i[0],-1*i[1]))\n",
    "        return r1\n",
    "    \n",
    "    #going for pos\n",
    "    elif label == 1:\n",
    "        for i in vocab:\n",
    "            r.append((i,co[vocab[i]]))\n",
    "        return sorted(r,key=lambda x: -x[1])[:n]\n",
    "    \n",
    "def parse_test_data(best_result, vocab):\n",
    "    ###TODO\n",
    "    list_t=[]\n",
    "    test_docs, test_labels = read_data(os.path.join('data','test'))\n",
    "    f=best_result[\"features\"]\n",
    "    minf=best_result[\"min_freq\"]\n",
    "    punch = best_result[\"punct\"]\n",
    "    for i in test_docs:\n",
    "        list_t.append(tokenize(i,punch))\n",
    "    X_test,new_vocab = vectorize(list_t,f,minf,vocab)\n",
    "    return test_docs,test_labels,X_test\n",
    "\n",
    "def print_top_misclassified(test_docs, test_labels, X_test, clf, n):\n",
    "    ###TODO\n",
    "    new_list=[]\n",
    "    p_1=clf.predict_proba(X_test)\n",
    "    p = clf.predict(X_test)\n",
    "    for i in range(0,len(p)):\n",
    "        d={}\n",
    "        if(p[i] != test_labels[i]):\n",
    "            if p[i] == 0:\n",
    "                d['test'] = test_docs[i]\n",
    "                d['true'] = p_1[i][0]\n",
    "                d['predicted'] = p[i]\n",
    "                d['label'] = test_labels[i]\n",
    "            else:\n",
    "                d['test'] = test_docs[i]\n",
    "                d['true'] = p_1[i][1]\n",
    "                d['predicted'] = p[i]\n",
    "                d['label'] = test_labels[i]\n",
    "            new_list.append(d)\n",
    "    new_list = sorted(new_list,key=lambda x:(-x['true']))[:n]\n",
    "    for i in new_list:\n",
    "        print('truth=%d predicted=%d proba=%.6f'%(i['label'],i['predicted'],i['true']))\n",
    "        print(i['test']+\"\\n\")\n",
    "        \n",
    "def main():\n",
    "    \"\"\"\n",
    "    Put it all together.\n",
    "    ALREADY DONE.\n",
    "    \"\"\"\n",
    "    feature_fns = [token_features, token_pair_features, lexicon_features]\n",
    "    # Download and read data.\n",
    "    download_data()\n",
    "    docs, labels = read_data(os.path.join('data', 'train'))\n",
    "    # Evaluate accuracy of many combinations\n",
    "    # of tokenization/featurization.\n",
    "    results = eval_all_combinations(docs, labels,\n",
    "                                    [True, False],\n",
    "                                    feature_fns,\n",
    "                                    [2,5,10])\n",
    "    # Print information about these results.\n",
    "    best_result = results[0]\n",
    "    worst_result = results[-1]\n",
    "    print('best cross-validation result:\\n%s' % str(best_result))\n",
    "    print('worst cross-validation result:\\n%s' % str(worst_result))\n",
    "    plot_sorted_accuracies(results)\n",
    "    print('\\nMean Accuracies per Setting:')\n",
    "    print('\\n'.join(['%s: %.5f' % (s,v) for v,s in mean_accuracy_per_setting(results)]))\n",
    "\n",
    "    # Fit best classifier.\n",
    "    clf, vocab = fit_best_classifier(docs, labels, results[0])\n",
    "\n",
    "    # Print top coefficients per class.\n",
    "    print('\\nTOP COEFFICIENTS PER CLASS:')\n",
    "    print('negative words:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 0, 5, vocab)]))\n",
    "    print('\\npositive words:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 1, 5, vocab)]))\n",
    "\n",
    "    # Parse test data\n",
    "    test_docs, test_labels, X_test = parse_test_data(best_result, vocab)\n",
    "\n",
    "    # Evaluate on test set.\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('testing accuracy=%f' %\n",
    "          accuracy_score(test_labels, predictions))\n",
    "\n",
    "    print('\\nTOP MISCLASSIFIED TEST DOCUMENTS:')\n",
    "    print_top_misclassified(test_docs, test_labels, X_test, clf, 5)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
